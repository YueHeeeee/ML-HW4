{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "X,y = utils.load_mat('data/spamTrain.mat')\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "\n",
    "val_indx = 3000\n",
    "Xval = X[val_indx:y.shape[0]]\n",
    "yval = yy[val_indx:y.shape[0]]\n",
    "\n",
    "X = X[0:val_indx]\n",
    "yy = yy[0:val_indx]\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test == 0] = -1\n",
    "\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.797 when sigma =  1\n",
      "Accuracy is  0.972 when sigma =  10\n",
      "Accuracy is  0.953 when sigma =  100\n",
      "Best accuracy =  0.972 Best sigma =  10\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# Should X be scaled? Yes\n",
    "# Should X be kernelized? Yes\n",
    "from linear_classifier import LinearSVM\n",
    "Best_accuracy = 0\n",
    "sigmas = [1,10,100]\n",
    "# choosing sigma\n",
    "for sigma in sigmas:\n",
    "    # compute the kernel (slow!)\n",
    "    K = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in X for x2 in X]).reshape(X.shape[0],X.shape[0])\n",
    "    # scale the kernelized data matrix\n",
    "    scaler = preprocessing.StandardScaler().fit(K)\n",
    "    scaleK = scaler.transform(K)\n",
    "    # add the intercept term\n",
    "    KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "\n",
    "    # seperating val set\n",
    "    # compute the kernel (slow!)\n",
    "    Kval = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in Xval for x2 in X]).reshape(Xval.shape[0],X.shape[0])\n",
    "    # scale the kernelized data matrix\n",
    "    scaler_val = preprocessing.StandardScaler().fit(Kval)\n",
    "    scaleK_val = scaler.transform(Kval)\n",
    "    # add the intercept term\n",
    "    KKval = np.vstack([np.ones((scaleK_val.shape[0],)),scaleK_val.T]).T \n",
    "\n",
    "    # seperating train set\n",
    "    # compute the kernel (slow!)\n",
    "    Ktest = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in X_test for x2 in X]).reshape(X_test.shape[0],X.shape[0])\n",
    "    # scale the kernelized data matrix\n",
    "    scaler_test = preprocessing.StandardScaler().fit(Ktest)\n",
    "    scaleK_test = scaler.transform(Ktest)\n",
    "    # add the intercept term\n",
    "    KKtest = np.vstack([np.ones((scaleK_test.shape[0],)),scaleK_test.T]).T \n",
    "        \n",
    "    svm = LinearSVM_twoclass()     \n",
    "    svm.train(KK,yy,learning_rate=1e-4,reg=100,num_iters=1000,verbose=False,batch_size=KK.shape[0])\n",
    "    predy = svm.predict(KKval)\n",
    "    accuracy = np.mean(predy == yval)\n",
    "    print(\"Accuracy is \",accuracy, \"when sigma = \",sigma)\n",
    "    if accuracy > Best_accuracy:\n",
    "        Best_accuracy, Best_sigma = accuracy, sigma\n",
    "print(\"Best accuracy = \", Best_accuracy, \"Best sigma = \", Best_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = Best_sigma\n",
    "# compute the kernel (slow!)\n",
    "K = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in X for x2 in X]).reshape(X.shape[0],X.shape[0])\n",
    "# scale the kernelized data matrix\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "# add the intercept term\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "\n",
    "# seperating val set\n",
    "# compute the kernel (slow!)\n",
    "Kval = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in Xval for x2 in X]).reshape(Xval.shape[0],X.shape[0])\n",
    "# scale the kernelized data matrix\n",
    "scaler_val = preprocessing.StandardScaler().fit(Kval)\n",
    "scaleK_val = scaler.transform(Kval)\n",
    "# add the intercept term\n",
    "KKval = np.vstack([np.ones((scaleK_val.shape[0],)),scaleK_val.T]).T \n",
    "\n",
    "# seperating train set\n",
    "# compute the kernel (slow!)\n",
    "Ktest = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in X_test for x2 in X]).reshape(X_test.shape[0],X.shape[0])\n",
    "# scale the kernelized data matrix\n",
    "scaler_test = preprocessing.StandardScaler().fit(Ktest)\n",
    "scaleK_test = scaler.transform(Ktest)\n",
    "# add the intercept term\n",
    "KKtest = np.vstack([np.ones((scaleK_test.shape[0],)),scaleK_test.T]).T \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.599 when C =  0.01\n",
      "Accuracy is  0.632 when C =  0.03\n",
      "Accuracy is  0.698 when C =  0.1\n",
      "Accuracy is  0.853 when C =  0.3\n",
      "Accuracy is  0.922 when C =  1\n",
      "Accuracy is  0.934 when C =  3\n",
      "Accuracy is  0.959 when C =  10\n",
      "Accuracy is  0.963 when C =  30\n",
      "Accuracy is  0.976 when C =  100\n",
      "Best accuracy =  0.976 Best C =  100\n"
     ]
    }
   ],
   "source": [
    "Best_accuracy, Best_C = 0, 0\n",
    "# what should C be?\n",
    "Cvals = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "for C in Cvals:\n",
    "    svm = LinearSVM_twoclass()     \n",
    "    svm.train(KK,yy,learning_rate=1e-4,reg=C,num_iters=1000,verbose=False,batch_size=KK.shape[0])\n",
    "    predy = svm.predict(KKval)\n",
    "    accuracy = np.mean(predy == yval)\n",
    "    print(\"Accuracy is \",accuracy, \"when C = \",C)\n",
    "    if accuracy > Best_accuracy:\n",
    "        Best_accuracy, Best_C = accuracy, C\n",
    "print(\"Best accuracy = \", Best_accuracy, \"Best C = \", Best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.685 when learning rate =  0.01\n",
      "Accuracy is  0.967 when learning rate =  0.001\n",
      "Accuracy is  0.977 when learning rate =  0.0001\n",
      "Accuracy is  0.962 when learning rate =  1e-05\n",
      "Best accuracy =  0.977 Best learning rate =  0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What should the learning rate be?\n",
    "Best_accuracy, Best_lr = 0, 0\n",
    "color = ['red','green','blue','yellow']\n",
    "Best_C =  100\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "fig = plt.figure()\n",
    "for lr in learning_rates:\n",
    "    svm = LinearSVM_twoclass()\n",
    "    J_theta = svm.train(KK,yy,learning_rate=lr,reg=Best_C,num_iters=1000,verbose=False,batch_size=KK.shape[0])\n",
    "    predy = svm.predict(KKval)\n",
    "    accuracy = np.mean(predy == yval)\n",
    "    print(\"Accuracy is \",accuracy, \"when learning rate = \",lr)\n",
    "    if accuracy > Best_accuracy:\n",
    "        Best_accuracy, Best_lr = accuracy,lr\n",
    "print(\"Best accuracy = \", Best_accuracy, \"Best learning rate = \", Best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20a80127978>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "J_reverse = J_theta.sort(reverse = True)\n",
    "plt.plot(range(len(J_theta)),J_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 99.937522\n",
      "iteration 100 / 2000: loss 67.396680\n",
      "iteration 200 / 2000: loss 49.893368\n",
      "iteration 300 / 2000: loss 14.022788\n",
      "iteration 400 / 2000: loss 14.110224\n",
      "iteration 500 / 2000: loss 37.372113\n",
      "iteration 600 / 2000: loss 10.804756\n",
      "iteration 700 / 2000: loss 10.141990\n",
      "iteration 800 / 2000: loss 9.100913\n",
      "iteration 900 / 2000: loss 34.392209\n",
      "iteration 1000 / 2000: loss 8.268447\n",
      "iteration 1100 / 2000: loss 7.589044\n",
      "iteration 1200 / 2000: loss 8.605159\n",
      "iteration 1300 / 2000: loss 7.219641\n",
      "iteration 1400 / 2000: loss 6.794903\n",
      "iteration 1500 / 2000: loss 7.059027\n",
      "iteration 1600 / 2000: loss 6.517537\n",
      "iteration 1700 / 2000: loss 6.709078\n",
      "iteration 1800 / 2000: loss 6.157251\n",
      "iteration 1900 / 2000: loss 7.479385\n",
      "Accuracy is  0.977 when number of iteration =  2000\n",
      "iteration 0 / 5000: loss 100.091277\n",
      "iteration 100 / 5000: loss 67.570644\n",
      "iteration 200 / 5000: loss 49.280568\n",
      "iteration 300 / 5000: loss 14.023691\n",
      "iteration 400 / 5000: loss 14.320165\n",
      "iteration 500 / 5000: loss 44.650569\n",
      "iteration 600 / 5000: loss 10.499012\n",
      "iteration 700 / 5000: loss 10.137939\n",
      "iteration 800 / 5000: loss 9.067875\n",
      "iteration 900 / 5000: loss 10.443340\n",
      "iteration 1000 / 5000: loss 8.120768\n",
      "iteration 1100 / 5000: loss 7.582928\n",
      "iteration 1200 / 5000: loss 9.375564\n",
      "iteration 1300 / 5000: loss 7.234633\n",
      "iteration 1400 / 5000: loss 6.787409\n",
      "iteration 1500 / 5000: loss 7.044427\n",
      "iteration 1600 / 5000: loss 6.465832\n",
      "iteration 1700 / 5000: loss 6.790449\n",
      "iteration 1800 / 5000: loss 6.193693\n",
      "iteration 1900 / 5000: loss 6.402464\n",
      "iteration 2000 / 5000: loss 5.847182\n",
      "iteration 2100 / 5000: loss 6.853637\n",
      "iteration 2200 / 5000: loss 5.573222\n",
      "iteration 2300 / 5000: loss 6.395732\n",
      "iteration 2400 / 5000: loss 5.323871\n",
      "iteration 2500 / 5000: loss 5.711269\n",
      "iteration 2600 / 5000: loss 5.096135\n",
      "iteration 2700 / 5000: loss 5.158740\n",
      "iteration 2800 / 5000: loss 27.425935\n",
      "iteration 2900 / 5000: loss 4.874640\n",
      "iteration 3000 / 5000: loss 4.896412\n",
      "iteration 3100 / 5000: loss 4.786462\n",
      "iteration 3200 / 5000: loss 4.696697\n",
      "iteration 3300 / 5000: loss 4.595935\n",
      "iteration 3400 / 5000: loss 4.386269\n",
      "iteration 3500 / 5000: loss 4.467503\n",
      "iteration 3600 / 5000: loss 5.404485\n",
      "iteration 3700 / 5000: loss 4.212308\n",
      "iteration 3800 / 5000: loss 4.264656\n",
      "iteration 3900 / 5000: loss 4.218378\n",
      "iteration 4000 / 5000: loss 4.154448\n",
      "iteration 4100 / 5000: loss 23.651329\n",
      "iteration 4200 / 5000: loss 3.916110\n",
      "iteration 4300 / 5000: loss 3.767552\n",
      "iteration 4400 / 5000: loss 3.979183\n",
      "iteration 4500 / 5000: loss 3.823397\n",
      "iteration 4600 / 5000: loss 3.599456\n",
      "iteration 4700 / 5000: loss 35.924348\n",
      "iteration 4800 / 5000: loss 3.590782\n",
      "iteration 4900 / 5000: loss 3.483329\n",
      "Accuracy is  0.866 when number of iteration =  5000\n",
      "iteration 0 / 10000: loss 99.399195\n",
      "iteration 100 / 10000: loss 67.259270\n",
      "iteration 200 / 10000: loss 49.414985\n",
      "iteration 300 / 10000: loss 14.020220\n",
      "iteration 400 / 10000: loss 14.183975\n",
      "iteration 500 / 10000: loss 45.811763\n",
      "iteration 600 / 10000: loss 10.472504\n",
      "iteration 700 / 10000: loss 10.194982\n",
      "iteration 800 / 10000: loss 9.022972\n",
      "iteration 900 / 10000: loss 30.157146\n",
      "iteration 1000 / 10000: loss 8.155467\n",
      "iteration 1100 / 10000: loss 7.699413\n",
      "iteration 1200 / 10000: loss 9.437535\n",
      "iteration 1300 / 10000: loss 7.239520\n",
      "iteration 1400 / 10000: loss 6.783221\n",
      "iteration 1500 / 10000: loss 7.191106\n",
      "iteration 1600 / 10000: loss 6.491305\n",
      "iteration 1700 / 10000: loss 7.706488\n",
      "iteration 1800 / 10000: loss 6.214254\n",
      "iteration 1900 / 10000: loss 7.246635\n",
      "iteration 2000 / 10000: loss 5.880506\n",
      "iteration 2100 / 10000: loss 11.424268\n",
      "iteration 2200 / 10000: loss 5.615537\n",
      "iteration 2300 / 10000: loss 7.242126\n",
      "iteration 2400 / 10000: loss 5.364826\n",
      "iteration 2500 / 10000: loss 17.598911\n",
      "iteration 2600 / 10000: loss 5.148084\n",
      "iteration 2700 / 10000: loss 5.275647\n",
      "iteration 2800 / 10000: loss 5.038059\n",
      "iteration 2900 / 10000: loss 4.933493\n",
      "iteration 3000 / 10000: loss 4.920854\n",
      "iteration 3100 / 10000: loss 4.837892\n",
      "iteration 3200 / 10000: loss 4.687145\n",
      "iteration 3300 / 10000: loss 4.508029\n",
      "iteration 3400 / 10000: loss 4.581641\n",
      "iteration 3500 / 10000: loss 4.480331\n",
      "iteration 3600 / 10000: loss 4.422199\n",
      "iteration 3700 / 10000: loss 4.184985\n",
      "iteration 3800 / 10000: loss 4.318940\n",
      "iteration 3900 / 10000: loss 4.192111\n",
      "iteration 4000 / 10000: loss 4.153027\n",
      "iteration 4100 / 10000: loss 3.855450\n",
      "iteration 4200 / 10000: loss 25.706006\n",
      "iteration 4300 / 10000: loss 3.831354\n",
      "iteration 4400 / 10000: loss 3.797737\n",
      "iteration 4500 / 10000: loss 3.788925\n",
      "iteration 4600 / 10000: loss 3.594147\n",
      "iteration 4700 / 10000: loss 4.615938\n",
      "iteration 4800 / 10000: loss 3.564847\n",
      "iteration 4900 / 10000: loss 3.477496\n",
      "iteration 5000 / 10000: loss 27.321983\n",
      "iteration 5100 / 10000: loss 3.449385\n",
      "iteration 5200 / 10000: loss 3.360795\n",
      "iteration 5300 / 10000: loss 4.517795\n",
      "iteration 5400 / 10000: loss 3.314532\n",
      "iteration 5500 / 10000: loss 3.318895\n",
      "iteration 5600 / 10000: loss 3.324362\n",
      "iteration 5700 / 10000: loss 3.214571\n",
      "iteration 5800 / 10000: loss 3.561295\n",
      "iteration 5900 / 10000: loss 3.120804\n",
      "iteration 6000 / 10000: loss 20.350253\n",
      "iteration 6100 / 10000: loss 3.118218\n",
      "iteration 6200 / 10000: loss 3.198378\n",
      "iteration 6300 / 10000: loss 3.090834\n",
      "iteration 6400 / 10000: loss 2.987840\n",
      "iteration 6500 / 10000: loss 3.118570\n",
      "iteration 6600 / 10000: loss 2.975540\n",
      "iteration 6700 / 10000: loss 3.104756\n",
      "iteration 6800 / 10000: loss 2.898097\n",
      "iteration 6900 / 10000: loss 3.065174\n",
      "iteration 7000 / 10000: loss 2.887403\n",
      "iteration 7100 / 10000: loss 2.836569\n",
      "iteration 7200 / 10000: loss 3.045606\n",
      "iteration 7300 / 10000: loss 2.842348\n",
      "iteration 7400 / 10000: loss 2.774664\n",
      "iteration 7500 / 10000: loss 2.769020\n",
      "iteration 7600 / 10000: loss 2.948098\n",
      "iteration 7700 / 10000: loss 2.713018\n",
      "iteration 7800 / 10000: loss 2.803000\n",
      "iteration 7900 / 10000: loss 3.936709\n",
      "iteration 8000 / 10000: loss 2.664738\n",
      "iteration 8100 / 10000: loss 2.830952\n",
      "iteration 8200 / 10000: loss 2.707466\n",
      "iteration 8300 / 10000: loss 2.602731\n",
      "iteration 8400 / 10000: loss 2.604599\n",
      "iteration 8500 / 10000: loss 13.218091\n",
      "iteration 8600 / 10000: loss 2.569442\n",
      "iteration 8700 / 10000: loss 2.784636\n",
      "iteration 8800 / 10000: loss 2.531359\n",
      "iteration 8900 / 10000: loss 5.612431\n",
      "iteration 9000 / 10000: loss 2.499624\n",
      "iteration 9100 / 10000: loss 2.520147\n",
      "iteration 9200 / 10000: loss 2.847872\n",
      "iteration 9300 / 10000: loss 2.463567\n",
      "iteration 9400 / 10000: loss 2.451958\n",
      "iteration 9500 / 10000: loss 2.687565\n",
      "iteration 9600 / 10000: loss 2.395653\n",
      "iteration 9700 / 10000: loss 2.548256\n",
      "iteration 9800 / 10000: loss 2.367028\n",
      "iteration 9900 / 10000: loss 2.506957\n",
      "Accuracy is  0.978 when number of iteration =  10000\n",
      "Best accuracy =  0.978 Best iteration number =  10000\n"
     ]
    }
   ],
   "source": [
    "# What should the number of iterations be? \n",
    "Best_accuracy, Best_ni = 0, 0\n",
    "num_iters = [2000, 5000, 10000]\n",
    "for ni in num_iters:\n",
    "    svm = LinearSVM_twoclass()\n",
    "    J_theta = svm.train(KK,yy,learning_rate=Best_lr,reg=Best_C,num_iters=ni,verbose=True,batch_size=KK.shape[0])\n",
    "    predy = svm.predict(KKval)\n",
    "    accuracy = np.mean(predy == yval)\n",
    "    print(\"Accuracy is \",accuracy, \"when number of iteration = \",ni)\n",
    "    if accuracy > Best_accuracy:\n",
    "        Best_accuracy, Best_ni = accuracy,ni\n",
    "print(\"Best accuracy = \", Best_accuracy, \"Best iteration number = \", Best_ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 10000: loss 2.987655\n",
      "iteration 100 / 10000: loss 1.972815\n",
      "iteration 200 / 10000: loss 1.399853\n",
      "iteration 300 / 10000: loss 1.140813\n",
      "iteration 400 / 10000: loss 0.993460\n",
      "iteration 500 / 10000: loss 0.893914\n",
      "iteration 600 / 10000: loss 0.821180\n",
      "iteration 700 / 10000: loss 0.767750\n",
      "iteration 800 / 10000: loss 0.724904\n",
      "iteration 900 / 10000: loss 0.689762\n",
      "iteration 1000 / 10000: loss 0.660427\n",
      "iteration 1100 / 10000: loss 0.634873\n",
      "iteration 1200 / 10000: loss 0.612135\n",
      "iteration 1300 / 10000: loss 0.592125\n",
      "iteration 1400 / 10000: loss 0.574676\n",
      "iteration 1500 / 10000: loss 0.558953\n",
      "iteration 1600 / 10000: loss 0.544344\n",
      "iteration 1700 / 10000: loss 0.530922\n",
      "iteration 1800 / 10000: loss 0.518835\n",
      "iteration 1900 / 10000: loss 0.507708\n",
      "iteration 2000 / 10000: loss 0.497417\n",
      "iteration 2100 / 10000: loss 0.487931\n",
      "iteration 2200 / 10000: loss 0.478963\n",
      "iteration 2300 / 10000: loss 0.470248\n",
      "iteration 2400 / 10000: loss 0.462266\n",
      "iteration 2500 / 10000: loss 0.454938\n",
      "iteration 2600 / 10000: loss 0.448152\n",
      "iteration 2700 / 10000: loss 0.441893\n",
      "iteration 2800 / 10000: loss 0.436255\n",
      "iteration 2900 / 10000: loss 0.430749\n",
      "iteration 3000 / 10000: loss 0.425476\n",
      "iteration 3100 / 10000: loss 0.420480\n",
      "iteration 3200 / 10000: loss 0.415731\n",
      "iteration 3300 / 10000: loss 0.411131\n",
      "iteration 3400 / 10000: loss 0.406801\n",
      "iteration 3500 / 10000: loss 0.402707\n",
      "iteration 3600 / 10000: loss 0.398817\n",
      "iteration 3700 / 10000: loss 0.395138\n",
      "iteration 3800 / 10000: loss 0.391732\n",
      "iteration 3900 / 10000: loss 0.388541\n",
      "iteration 4000 / 10000: loss 0.385365\n",
      "iteration 4100 / 10000: loss 0.382591\n",
      "iteration 4200 / 10000: loss 0.379609\n",
      "iteration 4300 / 10000: loss 0.376992\n",
      "iteration 4400 / 10000: loss 0.374454\n",
      "iteration 4500 / 10000: loss 0.372068\n",
      "iteration 4600 / 10000: loss 0.369857\n",
      "iteration 4700 / 10000: loss 0.367367\n",
      "iteration 4800 / 10000: loss 0.365103\n",
      "iteration 4900 / 10000: loss 0.363094\n",
      "iteration 5000 / 10000: loss 0.360660\n",
      "iteration 5100 / 10000: loss 0.358752\n",
      "iteration 5200 / 10000: loss 0.356578\n",
      "iteration 5300 / 10000: loss 0.354315\n",
      "iteration 5400 / 10000: loss 0.352363\n",
      "iteration 5500 / 10000: loss 0.350432\n",
      "iteration 5600 / 10000: loss 0.348533\n",
      "iteration 5700 / 10000: loss 0.346749\n",
      "iteration 5800 / 10000: loss 0.344983\n",
      "iteration 5900 / 10000: loss 0.343235\n",
      "iteration 6000 / 10000: loss 0.341523\n",
      "iteration 6100 / 10000: loss 0.340170\n",
      "iteration 6200 / 10000: loss 0.338206\n",
      "iteration 6300 / 10000: loss 0.336554\n",
      "iteration 6400 / 10000: loss 0.334970\n",
      "iteration 6500 / 10000: loss 0.333321\n",
      "iteration 6600 / 10000: loss 0.331829\n",
      "iteration 6700 / 10000: loss 0.330232\n",
      "iteration 6800 / 10000: loss 0.328749\n",
      "iteration 6900 / 10000: loss 0.327291\n",
      "iteration 7000 / 10000: loss 0.325888\n",
      "iteration 7100 / 10000: loss 0.324512\n",
      "iteration 7200 / 10000: loss 0.323158\n",
      "iteration 7300 / 10000: loss 0.321792\n",
      "iteration 7400 / 10000: loss 0.320500\n",
      "iteration 7500 / 10000: loss 0.319190\n",
      "iteration 7600 / 10000: loss 0.317877\n",
      "iteration 7700 / 10000: loss 0.316635\n",
      "iteration 7800 / 10000: loss 0.315326\n",
      "iteration 7900 / 10000: loss 0.314071\n",
      "iteration 8000 / 10000: loss 0.312902\n",
      "iteration 8100 / 10000: loss 0.311812\n",
      "iteration 8200 / 10000: loss 0.310447\n",
      "iteration 8300 / 10000: loss 0.309324\n",
      "iteration 8400 / 10000: loss 0.308142\n",
      "iteration 8500 / 10000: loss 0.307028\n",
      "iteration 8600 / 10000: loss 0.305956\n",
      "iteration 8700 / 10000: loss 0.304864\n",
      "iteration 8800 / 10000: loss 0.303718\n",
      "iteration 8900 / 10000: loss 0.302815\n",
      "iteration 9000 / 10000: loss 0.301699\n",
      "iteration 9100 / 10000: loss 0.300554\n",
      "iteration 9200 / 10000: loss 0.300200\n",
      "iteration 9300 / 10000: loss 0.298476\n",
      "iteration 9400 / 10000: loss 0.297502\n",
      "iteration 9500 / 10000: loss 0.296998\n",
      "iteration 9600 / 10000: loss 0.295571\n",
      "iteration 9700 / 10000: loss 0.294675\n",
      "iteration 9800 / 10000: loss 0.293693\n",
      "iteration 9900 / 10000: loss 0.292896\n",
      "Best accuracy =  0.967 when best C is 3 Best learning rate is 0.0001 Best iteration number is 10000\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's performance                       #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "Best_C = 3\n",
    "svm = LinearSVM_twoclass()\n",
    "J_theta = svm.train(KK,yy,learning_rate=Best_lr,reg=Best_C,num_iters=ni,verbose=True,batch_size=KK.shape[0])\n",
    "predy = svm.predict(KKtest)\n",
    "accuracy = np.mean(predy == yy_test)\n",
    "print(\"Best accuracy = \", accuracy, \"when best C is\", Best_C, \"Best learning rate is\", Best_lr, \"Best iteration number is\", Best_ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 15 word that are predictive of spam\n",
      "our\n",
      "click\n",
      "remov\n",
      "pleas\n",
      "here\n",
      "your\n",
      "we\n",
      "receiv\n",
      "nbsp\n",
      "free\n",
      "inform\n",
      "dollarnumb\n",
      "below\n",
      "offer\n",
      "email\n",
      "top 15 word that are predictive of ham\n",
      "httpaddr\n",
      "but\n",
      "wrote\n",
      "it\n",
      "date\n",
      "user\n",
      "that\n",
      "my\n",
      "url\n",
      "thei\n",
      "there\n",
      "emailaddr\n",
      "which\n",
      "group\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "print(\"top 15 word that are predictive of spam\")\n",
    "XXX = np.dot(svm.theta[1:],X).argsort()\n",
    "for i in XXX[-15:][::-1]:\n",
    "    print(words[i + 1])\n",
    "print(\"top 15 word that are predictive of ham\")\n",
    "for i in XXX[:15][::1]:\n",
    "    print(words[i + 1])\n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
